{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score, f1_score,\\\n",
    "                            hamming_loss, multilabel_confusion_matrix, precision_score, recall_score\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier, BaggingClassifier, StackingClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.pipeline import Pipeline\n",
    "from stop_words import get_stop_words\n",
    "from uk_stemmer import UkStemmer\n",
    "import tokenize_uk\n",
    "import matplotlib.pyplot as plt \n",
    "from nltk import stem\n",
    "import pymorphy2\n",
    "import razdel\n",
    "import dateparser\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading news in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ukr_news_path = '../train_data/combined_ukr.csv'\n",
    "ru_news_path = '../train_data/combined_ru.csv'\n",
    "\n",
    "comb_ukr = pd.read_csv(ukr_news_path)\n",
    "comb_ru = pd.read_csv(ru_news_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add automaticaly categorized news to ones categorized by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_samples_to_df(from_df, to_df, n_samples=200):\n",
    "    for cat in from_df.category.unique():\n",
    "        if (from_df.category==cat).sum() > n_samples-1:\n",
    "            samp = (from_df[from_df.category==cat]).sample(n_samples)\n",
    "        else:\n",
    "            samp = from_df[from_df.category==cat]\n",
    "        to_df = pd.concat([to_df, samp])\n",
    "\n",
    "    to_df = to_df.drop_duplicates('link')\n",
    "    to_df['all_text'] = to_df.title.str.cat(to_df.text, sep='\\n')\n",
    "    return to_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save sample news to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(news, domains = [], sample_size = 20, month=0, lang = '', output_file = '../data/new_samples.csv'):\n",
    "    news_samples = []\n",
    "    domains = news.domain.unique() if not domains else domains\n",
    "    lang_mask = news.lang==lang if lang else True\n",
    "    period_mask = news.datetime.dt.month>month\n",
    "    for d in domains:\n",
    "        news_samples.append(news[(news.domain==d)&period_mask&lang_mask].sample(sample_size))\n",
    "    news_samples = pd.concat(news_samples)\n",
    "    news_samples.to_csv(output_file, index=False)\n",
    "    return news_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of stop words, stemmer and lemmatizer for ukrainian and russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ukr = get_stop_words('uk')\n",
    "stop_words_ru = get_stop_words('ru')\n",
    "\n",
    "additional_sw_ukr_path = '../dicts/more_stop_words_ukr.txt'\n",
    "additional_sw_ru_path = '../dicts/more_stop_words_ru.txt'\n",
    "\n",
    "with open(additional_sw_ukr_path) as f:\n",
    "    more_stopwords_ukr = [w.strip() for w in f.readlines()]\n",
    "    \n",
    "with open(additional_sw_ru_path) as f:\n",
    "    more_stopwords_ru = [w.strip() for w in f.readlines()]\n",
    "    \n",
    "stop_words_ukr += more_stopwords_ukr\n",
    "stop_words_ru += more_stopwords_ru\n",
    "\n",
    "stemmer_ru = stem.snowball.SnowballStemmer(\"russian\") \n",
    "stemmer_ukr = UkStemmer()\n",
    "\n",
    "stemmed_stopwords_ukr = set([stemmer_ukr.stem_word(word) for word in stop_words_ukr])\n",
    "stemmed_stopwords_ru = set([stemmer_ru.stem(word) for word in stop_words_ru])\n",
    "\n",
    "morph_ru = pymorphy2.MorphAnalyzer(lang='ru')\n",
    "morph_ukr = pymorphy2.MorphAnalyzer(lang='uk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for custom tokenizers for ukrainian an russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = re.compile(r'\\w{2,}')\n",
    "\n",
    "stem_ukr = {\"tokenize\": tokenize_uk.tokenize_words,\n",
    "            \"stem\": stemmer_ukr.stem_word, \n",
    "            \"stop_words\": stemmed_stopwords_ukr,\n",
    "           \"token_pattern\": token_pattern}\n",
    "\n",
    "stem_ru = {\"tokenize\": lambda text: [_.text for _ in razdel.tokenize(text)],\n",
    "            \"stem\": stemmer_ru.stem, \n",
    "            \"stop_words\": stemmed_stopwords_ru,\n",
    "           \"token_pattern\": token_pattern}\n",
    "\n",
    "lemm_ukr = {\"tokenize\": tokenize_uk.tokenize_words,\n",
    "            \"stem\": lambda w: morph_ukr.parse(w)[0].normal_form, \n",
    "            \"stop_words\": stop_words_ukr,\n",
    "           \"token_pattern\": token_pattern}\n",
    "\n",
    "lemm_ru = {\"tokenize\": lambda text: [_.text for _ in razdel.tokenize(text)],\n",
    "            \"stem\": lambda w: morph_ru.parse(w)[0].normal_form, \n",
    "            \"stop_words\": stop_words_ru,\n",
    "           \"token_pattern\": token_pattern}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to split data, get custom tokenizer and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(text, label, test_size):\n",
    "    return train_test_split(text, label, test_size=test_size, stratify = label)\n",
    "\n",
    "\n",
    "def my_tokenizer(text, tokenize, stem, stop_words, token_pattern):\n",
    "    tokens = tokenize(text)\n",
    "    stemmed = [stem(tok) for tok in tokens if ( len(tok)>1 and\n",
    "                                        token_pattern.match(tok) )]\n",
    "    return [tok for tok in stemmed if tok not in stop_words]\n",
    "\n",
    "\n",
    "def get_features(vectorizer, X_train, X_test):\n",
    "    vectorizer.fit(X_train)\n",
    "    try:\n",
    "#         works only for vectorized without lambda\n",
    "        pickle.dump(vectorizer, open(\"vectorizer.pkl\", \"wb\"))\n",
    "    except:\n",
    "        pass\n",
    "    X_train = vectorizer.transform(X_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    return X_train, X_test, vectorizer.vocabulary_, vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single function to get transformed and oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_transform(df, text_col, label_col, tok_params, \n",
    "                         max_df=0.9, min_df=3, ngram_range=(1,2), test_size = 0.3):\n",
    "#     split data\n",
    "    X_train, X_test, y_train, y_test = split_data(df, df[label_col], test_size)\n",
    "    \n",
    "#     initialize vectorizer\n",
    "    vectorizer = TfidfVectorizer(tokenizer = lambda text: my_tokenizer(text, **tok_params), \n",
    "                             max_df=max_df, min_df=min_df, ngram_range=ngram_range)\n",
    "    \n",
    "#     transform text\n",
    "    X_train_vec, X_test_vec, vocabulary, feature_names = get_features(vectorizer, X_train[text_col], X_test[text_col])\n",
    "    \n",
    "#     oversample training set (make all categories as big as the largest one)\n",
    "    oversample = RandomOverSampler()\n",
    "    X_over, y_over = oversample.fit_resample(X_train_vec, y_train)\n",
    "    \n",
    "    return X_train, X_test, X_train_vec, X_test_vec, y_train, y_test, X_over, y_over, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oksana/Dev/TextClassification/venv/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, \\\n",
    "X_train_vec, X_test_vec, \\\n",
    "y_train, y_test, X_over, y_over, feature_names = train_test_transform(comb_ru.iloc[:1000], 'all_text', 'category', stem_ru)                                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check top features for every category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features(X_train, y_train, feature_names, N=10):\n",
    "    for cat in y_train.unique():\n",
    "        features_chi2 = chi2(X_train, y_train==cat)\n",
    "        indices = np.argsort(features_chi2[0])\n",
    "        features = np.array(feature_names)[indices]\n",
    "        unigrams = [v for v in features if len(v.split(' ')) == 1]\n",
    "        bigrams = [v for v in features if len(v.split(' ')) == 2]\n",
    "        print(\"# '{}':\".format(cat))\n",
    "        print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "        print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'Коронавірус':\n",
      "  . Most correlated unigrams:\n",
      ". кількіст\n",
      ". хвор\n",
      ". інфікован\n",
      ". доб\n",
      ". померл\n",
      ". пацієнт\n",
      ". одуж\n",
      ". covid\n",
      ". випадк\n",
      ". коронавірус\n",
      "# 'Спорт':\n",
      "  . Most correlated unigrams:\n",
      ". клуб\n",
      ". футболіст\n",
      ". тренер\n",
      ". команд\n",
      ". чемпіон\n",
      ". шахтар\n",
      ". чемпіонат\n",
      ". ліг\n",
      ". динам\n",
      ". матч\n",
      "# 'Міжнародна політика':\n",
      "  . Most correlated unigrams:\n",
      ". президент\n",
      ". g7\n",
      ". вибор\n",
      ". гонконг\n",
      ". байден\n",
      ". трамп\n",
      ". сша\n",
      ". лукашенк\n",
      ". білорус\n",
      ". санкці\n",
      "# 'Світ':\n",
      "  . Most correlated unigrams:\n",
      ". протестувальник\n",
      ". принц\n",
      ". хабаровськ\n",
      ". землетрус\n",
      ". протест\n",
      ". путін\n",
      ". бейрут\n",
      ". угорщин\n",
      ". білорус\n",
      ". лукашенк\n",
      "# 'Війна/Донбас':\n",
      "  . Most correlated unigrams:\n",
      ". великокаліберн\n",
      ". кулемет\n",
      ". вогн\n",
      ". міномет\n",
      ". калібр\n",
      ". оос\n",
      ". гранатомет\n",
      ". донбас\n",
      ". обстріл\n",
      ". бойовик\n",
      "# 'Інциденти':\n",
      "  . Most correlated unigrams:\n",
      ". кримінальн\n",
      ". дтп\n",
      ". зловмисник\n",
      ". ст\n",
      ". правоохоронц\n",
      ". поліц\n",
      ". чоловік\n",
      ". інцидент\n",
      ". вибух\n",
      ". поліці\n",
      "# 'Політика':\n",
      "  . Most correlated unigrams:\n",
      ". нардеп\n",
      ". верховн\n",
      ". політик\n",
      ". народ\n",
      ". депутат\n",
      ". посад\n",
      ". рад\n",
      ". порошенк\n",
      ". зеленськ\n",
      ". слуг\n",
      "# 'Карантин':\n",
      "  . Most correlated unigrams:\n",
      ". заклад\n",
      ". регіон\n",
      ". адаптивн\n",
      ". зелен\n",
      ". жовт\n",
      ". школ\n",
      ". помаранчев\n",
      ". карантин\n",
      ". червон\n",
      ". зон\n",
      "# 'Суспільство':\n",
      "  . Most correlated unigrams:\n",
      ". ре\n",
      ". антоненк\n",
      ". абітурієнт\n",
      ". уніан\n",
      ". поїзд\n",
      ". квитк\n",
      ". суспільств\n",
      ". пробн\n",
      ". оцінюванн\n",
      ". зно\n",
      "# 'Шоу-бізнес':\n",
      "  . Most correlated unigrams:\n",
      ". instagram\n",
      ". показал\n",
      ". даш\n",
      ". артистк\n",
      ". каменськ\n",
      ". пісн\n",
      ". красун\n",
      ". фільм\n",
      ". зірк\n",
      ". співачк\n",
      "# 'Економіка':\n",
      "  . Most correlated unigrams:\n",
      ". цін\n",
      ". нбу\n",
      ". валют\n",
      ". мвф\n",
      ". євр\n",
      ". млрд\n",
      ". долар\n",
      ". грн\n",
      ". гривн\n",
      ". курс\n",
      "# 'Інше':\n",
      "  . Most correlated unigrams:\n",
      ". радим\n",
      ". банком\n",
      ". кан\n",
      ". проститут\n",
      ". докладніш\n",
      ". новин\n",
      ". клят\n",
      ". green\n",
      ". пишем\n",
      ". випуск\n",
      "# 'Життя/lifestyle':\n",
      "  . Most correlated unigrams:\n",
      ". фотогалере\n",
      ". іменин\n",
      ". instagram\n",
      ". сар\n",
      ". images\n",
      ". знак\n",
      ". свят\n",
      ". зодіак\n",
      ". getty\n",
      ". сукн\n",
      "# 'Погода':\n",
      "  . Most correlated unigrams:\n",
      ". тепл\n",
      ". опад\n",
      ". температур\n",
      ". вноч\n",
      ". дощ\n",
      ". градус\n",
      ". синоптик\n",
      ". погод\n",
      ". гроз\n",
      ". вден\n",
      "# 'Технології':\n",
      "  . Most correlated unigrams:\n",
      ". супутник\n",
      ". xiaomi\n",
      ". nasа\n",
      ". інтелект\n",
      ". марс\n",
      ". космічн\n",
      ". apple\n",
      ". техн\n",
      ". смартфон\n",
      ". вчен\n"
     ]
    }
   ],
   "source": [
    "get_top_features(X_train_vec, y_train, feature_names, N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check top word features (general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['океан', 'трагічн', 'чемпіонк', 'смерт', 'загинул', 'спортсменк',\n",
       "       'човн', 'похід', 'перетнут', 'мертв', 'оголосит', 'змаган', 'світ',\n",
       "       'фейсбук', 'намагал', 'guardian', 'берег', 'спортсмен', 'відом',\n",
       "       'унікальн', 'рекорд', 'нагадуєм', 'встановит', 'завершит',\n",
       "       'страшн', 'загинув', 'хотіл', 'написал', 'the', 'тіл'],\n",
       "      dtype='<U19')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_array = np.array(feature_names)\n",
    "tfidf_sorting = np.argsort(X_train_vec.toarray()).flatten()[::-1]\n",
    "n = 30\n",
    "feature_array[tfidf_sorting][:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_train, X_test, y_train, y_test, n_splits=4):\n",
    "    clf = model.fit(X_train, y_train)\n",
    "    predicted = clf.predict(X_test)\n",
    "    plot_confusion_matrix(clf, X_test, y_test, xticks_rotation = 'vertical') \n",
    "    plt.show() \n",
    "    cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=1)\n",
    "    cv_score = cross_val_score(model, X_train, y_train, cv=cv, scoring = 'f1_weighted')\n",
    "    mcv = cv_score.mean()\n",
    "\n",
    "    print('Weighted f1_score:', f1_score(y_test,predicted, average = 'weighted'))\n",
    "    print('Micro f1_score:', f1_score(y_test,predicted, average = 'micro'))\n",
    "    print('Macro f1_score:', f1_score(y_test,predicted, average = 'macro'))\n",
    "    print('Accuracy:', accuracy_score(y_test,predicted))\n",
    "    print('Recall:', recall_score(y_test,predicted, average = 'weighted'))\n",
    "    print('Precision:', precision_score(y_test,predicted, average = 'weighted'))\n",
    "    print('CV_scores:', cv_score)\n",
    "    print('Mean cv_score:', mcv)\n",
    "    return clf, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model on original data\n",
    "model, predicted = test_model(LinearSVC(), X_train_vec, X_test_vec, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model on oversampled data - ignore CV_score, because there can be same samples in train and test parts\n",
    "model, predicted = test_model(LinearSVC(), X_over, X_test_vec, y_over, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save misclassified items to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes = X_test[y_test != predicted]\n",
    "mistakes['predicted'] = predicted[y_test != predicted]\n",
    "mistakes[['category', 'predicted', 'all_text', 'link']].to_csv('../data/misclassified_news.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check accuracy by cross validation on whole set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7827218949908926"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_stem_ukr = TfidfVectorizer(tokenizer = lambda text: my_tokenizer(text, **stem_ukr), \n",
    "                             max_df=0.9, min_df = 3, ngram_range=(1,2))\n",
    "\n",
    "pipeline = Pipeline([('vectorizer', tfidf_stem_ukr),  ('svc', LinearSVC())])\n",
    "cv = RepeatedStratifiedKFold(n_splits=4, n_repeats=2)\n",
    "cv_score = cross_val_score(pipeline, comb_ukr.all_text, comb_ukr.category, cv=cv, scoring='f1_weighted')\n",
    "cv_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best model parameters with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_features': 'sqrt'}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'criterion':['gini', 'entropy'], 'max_features':['auto', 'sqrt', 'log2']}\n",
    "svc = RandomForestClassifier()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "clf.fit(X_train_vec, y_train)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other variations for data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be able to pickle vectorizer you need to get rid of lambda\n",
    "# i didn't find a better way than defining two separate tokenizers for ukrainian and russian\n",
    "\n",
    "def tokenizer_ukr(text, tokenize=tokenize_uk.tokenize_words, \n",
    "                  stem=stemmer_ukr.stem_word, stop_words=stemmed_stopwords_ukr, token_pattern=token_pattern):\n",
    "    tokens = tokenize(text)\n",
    "    stemmed = [stem(tok) for tok in tokens if ( len(tok)>1 and\n",
    "                                        token_pattern.match(tok) )]\n",
    "    return [tok for tok in stemmed if tok not in stop_words]\n",
    "\n",
    "\n",
    "def tokenizer_ru(text, tokenize=lambda text: [_.text for _ in razdel.tokenize(text)], \n",
    "                 stem=stemmer_ru.stem, stop_words=stemmed_stopwords_ru, token_pattern=token_pattern):\n",
    "    tokens = tokenize(text)\n",
    "    stemmed = [stem(tok) for tok in tokens if ( len(tok)>1 and\n",
    "                                        token_pattern.match(tok) )]\n",
    "    return [tok for tok in stemmed if tok not in stop_words]\n",
    "\n",
    "# now you can fit and save to file\n",
    "tfidf_stem_ukr = TfidfVectorizer(tokenizer = tokenizer_ukr, \n",
    "                             max_df=0.9, min_df = 3, ngram_range=(1,2))\n",
    "\n",
    "\n",
    "ukr_stem_vec = tfidf_stem_ukr.fit_transform(comb_ukr.all_text)\n",
    "pickle.dump(tfidf_stem_ukr, open(\"vectorizer_ukr.pkl\", \"wb\"))\n",
    "pickle.dump(ukr_stem_vec, open(\"transformed_text_ukr.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(ukr_stem_vec, comb_ukr.category)\n",
    "pickle.dump(clf, open(\"SVC_ukr.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option with pipeline - to be able to use directly with text\n",
    "pipeline = Pipeline(steps=[('vectorizer', tfidf_stem_ukr), ('svc', LinearSVC())])\n",
    "pipeline.fit(comb_ukr.all_text, comb_ukr.category)\n",
    "\n",
    "# save model to file\n",
    "pickle.dump(pipeline, open(\"pipeline_ukr.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline option\n",
    "september = pd.read_csv('../data/sept_categorized.csv')\n",
    "september['all_text'] = september.title.str.cat(september.text, sep='\\n', na_rep='')\n",
    "\n",
    "model = pickle.load(open(\"pipeline_ukr.pkl\", \"rb\"))\n",
    "\n",
    "predicted = model.predict(september[september.lang=='uk'].all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or two steps\n",
    "model = pickle.load(open('SVC_ukr.pkl', 'rb'))\n",
    "tfidf = pickle.load(open(\"vectorizer_ukr.pkl\", \"rb\"))\n",
    "\n",
    "transformed = tfidf.transform(september[september.lang=='uk'].all_text)\n",
    "predicted = model.predict(transformed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
